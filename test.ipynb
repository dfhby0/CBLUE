{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "sys.path.append('.')\r\n",
    "import argparse\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "from transformers import BertTokenizer, BertModel, AlbertModel, BertForSequenceClassification, \\\r\n",
    "    AlbertForSequenceClassification\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "from cblue.models import CDNForCLSModel\r\n",
    "from cblue.trainer import CDNForCLSTrainer, CDNForNUMTrainer\r\n",
    "from cblue.utils import init_logger, seed_everything\r\n",
    "from cblue.data import CDNDataset, CDNDataProcessor\r\n",
    "from cblue.models import save_zen_model, ZenModel, ZenForSequenceClassification, ZenNgramDict\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DATA_DIR=\"CBLUEDatasets\"\r\n",
    "\r\n",
    "TASK_NAME=\"cdn\"\r\n",
    "MODEL_TYPE=\"bert\"\r\n",
    "MODEL_DIR=\"data/model_data\"\r\n",
    "MODEL_NAME=\"chinese-macbert-large\"\r\n",
    "OUTPUT_DIR=\"data/output\"\r\n",
    "RESULT_OUTPUT_DIR=\"data/result_output\"\r\n",
    "\r\n",
    "MAX_LENGTH=64\r\n",
    "\r\n",
    "RECALL_K=200\r\n",
    "NUM_NEGATIVE_SAMPLES=5\r\n",
    "DO_AUGMENT=6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "output_dir = OUTPUT_DIR\r\n",
    "task_name = TASK_NAME\r\n",
    "model_name = MODEL_NAME\r\n",
    "model_type = MODEL_TYPE\r\n",
    "model_dir = MODEL_DIR\r\n",
    "data_dir = DATA_DIR\r\n",
    "recall_k = RECALL_K\r\n",
    "num_neg = NUM_NEGATIVE_SAMPLES\r\n",
    "do_aug = 6\r\n",
    "\r\n",
    "seed = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if not os.path.exists(output_dir):\r\n",
    "    os.mkdir(output_dir)\r\n",
    "output_dir = os.path.join(output_dir, task_name)\r\n",
    "if not os.path.exists(output_dir):\r\n",
    "    os.mkdir(output_dir)\r\n",
    "output_dir = os.path.join(output_dir, model_name)\r\n",
    "if not os.path.exists(output_dir):\r\n",
    "    os.mkdir(output_dir)\r\n",
    "    \r\n",
    "\r\n",
    "logger = init_logger(os.path.join(output_dir, f'{task_name}_{model_name}.log'))\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "device = device\r\n",
    "seed_everything(seed)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "MODEL_CLASS = {\r\n",
    "    'bert': (BertTokenizer, BertModel),\r\n",
    "    'roberta': (BertTokenizer, BertModel),\r\n",
    "    'albert': (BertTokenizer, AlbertModel),\r\n",
    "    'zen': (BertTokenizer, ZenModel)\r\n",
    "}\r\n",
    "\r\n",
    "CLS_MODEL_CLASS = {\r\n",
    "    'bert': BertForSequenceClassification,\r\n",
    "    'roberta': BertForSequenceClassification,\r\n",
    "    'albert': AlbertForSequenceClassification,\r\n",
    "    'zen': ZenForSequenceClassification\r\n",
    "}\r\n",
    "tokenizer_class, model_class = MODEL_CLASS[model_type]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "logger.info('Training CLS model...')\r\n",
    "tokenizer = tokenizer_class.from_pretrained('./data/model_data/chinese-macbert-large',local_files_only = True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "09/14/2021 15:44:55 - INFO - root -   Training CLS model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ngram_dict = None\r\n",
    "\r\n",
    "data_processor = CDNDataProcessor(root=data_dir, recall_k=recall_k,\r\n",
    "                                    negative_sample=num_neg)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "09/14/2021 15:45:02 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\DD045C~1.DES\\AppData\\Local\\Temp\\jieba.cache\n",
      "09/14/2021 15:45:04 - DEBUG - jieba -   Dumping model to file cache C:\\Users\\DD045C~1.DES\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.797 seconds.\n",
      "09/14/2021 15:45:04 - DEBUG - jieba -   Loading model cost 1.797 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "09/14/2021 15:45:04 - DEBUG - jieba -   Prefix dict has been built successfully.\n",
      "09/14/2021 15:45:07 - INFO - gensim.corpora.dictionary -   adding document #0 to Dictionary(0 unique tokens: [])\n",
      "09/14/2021 15:45:07 - INFO - gensim.corpora.dictionary -   adding document #10000 to Dictionary(4399 unique tokens: ['霍乱', ',', '01', '型', '所致']...)\n",
      "09/14/2021 15:45:07 - INFO - gensim.corpora.dictionary -   adding document #20000 to Dictionary(6737 unique tokens: ['霍乱', ',', '01', '型', '所致']...)\n",
      "09/14/2021 15:45:07 - INFO - gensim.corpora.dictionary -   adding document #30000 to Dictionary(8736 unique tokens: ['霍乱', ',', '01', '型', '所致']...)\n",
      "09/14/2021 15:45:08 - INFO - gensim.corpora.dictionary -   built Dictionary(10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...) from 37879 documents (total 204725 corpus positions)\n",
      "09/14/2021 15:45:08 - INFO - gensim.utils -   Dictionary lifecycle event {'msg': \"built Dictionary(10473 unique tokens: ['霍乱', ',', '01', '型', '所致']...) from 37879 documents (total 204725 corpus positions)\", 'datetime': '2021-09-14T15:45:08.019217', 'gensim': '4.1.0', 'python': '3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "09/14/2021 15:45:08 - INFO - gensim.models.tfidfmodel -   collecting document frequencies\n",
      "09/14/2021 15:45:08 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #0\n",
      "09/14/2021 15:45:08 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #10000\n",
      "09/14/2021 15:45:08 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #20000\n",
      "09/14/2021 15:45:08 - INFO - gensim.models.tfidfmodel -   PROGRESS: processing document #30000\n",
      "09/14/2021 15:45:08 - INFO - gensim.utils -   TfidfModel lifecycle event {'msg': 'calculated IDF weights for 37879 documents and 10473 features (196761 matrix non-zeros)', 'datetime': '2021-09-14T15:45:08.630218', 'gensim': '4.1.0', 'python': '3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'initialize'}\n",
      "09/14/2021 15:45:08 - INFO - gensim.similarities.docsim -   creating sparse index\n",
      "09/14/2021 15:45:08 - INFO - gensim.matutils -   creating sparse matrix from corpus\n",
      "09/14/2021 15:45:08 - INFO - gensim.matutils -   PROGRESS: at document #0\n",
      "09/14/2021 15:45:09 - INFO - gensim.matutils -   PROGRESS: at document #10000\n",
      "09/14/2021 15:45:10 - INFO - gensim.matutils -   PROGRESS: at document #20000\n",
      "09/14/2021 15:45:10 - INFO - gensim.matutils -   PROGRESS: at document #30000\n",
      "09/14/2021 15:45:11 - INFO - gensim.similarities.docsim -   created <37879x10473 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 196761 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_samples, recall_orig_train_samples, recall_orig_train_samples_scores = data_processor.get_train_sample(dtype='cls', do_augment=do_aug)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6000/6000 [15:57<00:00,  6.27it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "eval_samples, recall_orig_eval_samples, recall_orig_train_samples_scores = data_processor.get_dev_sample(dtype='cls', do_augment=do_aug)\r\n",
    "\r\n",
    "if data_processor.recall:\r\n",
    "    logger.info('first recall score: %s', data_processor.recall)\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data_processor' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-98edc5846b36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_orig_eval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_orig_train_samples_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dev_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cls'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_augment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_aug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'first recall score: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_processor' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "train_dataset = CDNDataset(train_samples, data_processor, dtype='cls', mode='train')\r\n",
    "eval_dataset = CDNDataset(eval_samples, data_processor, dtype='cls', mode='eval')\r\n",
    "\r\n",
    "model = CDNForCLSModel(model_class, encoder_path=os.path.join(model_dir, model_name),\r\n",
    "                        num_labels=data_processor.num_labels_cls)\r\n",
    "cls_model_class = CLS_MODEL_CLASS[model_type]\r\n",
    "trainer = CDNForCLSTrainer(args=args, model=model, data_processor=data_processor,\r\n",
    "                            tokenizer=tokenizer, train_dataset=train_dataset, eval_dataset=eval_dataset,\r\n",
    "                            logger=logger, recall_orig_eval_samples=recall_orig_eval_samples,\r\n",
    "                            model_class=cls_model_class, recall_orig_eval_samples_scores=recall_orig_train_samples_scores,\r\n",
    "                            ngram_dict=ngram_dict)\r\n",
    "\r\n",
    "global_step, best_step = trainer.train()\r\n",
    "\r\n",
    "model = CDNForCLSModel(model_class, encoder_path=os.path.join(output_dir, f'checkpoint-{best_step}'),\r\n",
    "                        num_labels=data_processor.num_labels_cls)\r\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, f'checkpoint-{best_step}', 'pytorch_model.pt')))\r\n",
    "tokenizer = tokenizer_class.from_pretrained(os.path.join(output_dir, f'checkpoint-{best_step}'))\r\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'pytorch_model_cls.pt'))\r\n",
    "if not os.path.exists(os.path.join(output_dir, 'cls')):\r\n",
    "    os.mkdir(os.path.join(output_dir, 'cls'))\r\n",
    "\r\n",
    "if model_type == 'zen':\r\n",
    "    save_zen_model(os.path.join(output_dir, 'cls'), model.encoder, tokenizer, ngram_dict, args)\r\n",
    "else:\r\n",
    "    model.encoder.save_pretrained(os.path.join(output_dir, 'cls'))\r\n",
    "\r\n",
    "tokenizer.save_vocabulary(save_directory=os.path.join(output_dir, 'cls'))\r\n",
    "logger.info('Saving models checkpoint to %s', os.path.join(output_dir, 'cls'))\r\n",
    "\r\n",
    "logger.info('Training NUM model...')\r\n",
    "logging_steps = 30\r\n",
    "save_steps = 30\r\n",
    "train_samples = data_processor.get_train_sample(dtype='num', do_augment=1)\r\n",
    "eval_samples = data_processor.get_dev_sample(dtype='num')\r\n",
    "train_dataset = CDNDataset(train_samples, data_processor, dtype='num', mode='train')\r\n",
    "eval_dataset = CDNDataset(eval_samples, data_processor, dtype='num', mode='eval')\r\n",
    "\r\n",
    "cls_model_class = CLS_MODEL_CLASS[model_type]\r\n",
    "model = cls_model_class.from_pretrained(os.path.join(model_dir, model_name),\r\n",
    "                                        num_labels=data_processor.num_labels_num)\r\n",
    "trainer = CDNForNUMTrainer(args=args, model=model, data_processor=data_processor,\r\n",
    "                            tokenizer=tokenizer, train_dataset=train_dataset, eval_dataset=eval_dataset,\r\n",
    "                            logger=logger, model_class=cls_model_class, ngram_dict=ngram_dict)\r\n",
    "\r\n",
    "global_step, best_step = trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e9b2c1aadb943414bf8548eb38ea62af0e335697df872ef1f8865006ede8166a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}